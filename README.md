# lab3_Rozy_Patel/Assignment 2
# Penguin Species Prediction API 

This project uses an XGBoost model deployed via FastAPI to predict penguin species based on physical features.

This project builds and deploys a machine learning pipeline to classify penguin species using the Palmer Penguins dataset. It includes:

- Model training with XGBoost

- A FastAPI app to serve predictions

- Input validation using Pydantic Enums

- Logging, error handling, and testing

- Project environment managed with uv

  Install Dependencies : uv pip install -r requirements.txt
  Start the API: uvicorn app.main:app --reload

Video Link
https://youtu.be/nxYx-tK8hsM

https://youtu.be/ulL62phMwK0

# Assignment 2
# Penguin Species Classification API

A production-ready FastAPI application that predicts penguin species (Adelie, Chinstrap, Gentoo) based on physical measurements using XGBoost machine learning with Google Cloud Storage integration and containerized deployment.

## Project Overview

This project implements a machine learning API that classifies penguin species using the Palmer Penguins dataset. The API accepts penguin physical measurements and returns species predictions with high accuracy.

### Key Features

- ** High Accuracy ML Model **: XGBoost classifier trained on Palmer Penguins dataset
- ** FastAPI REST API**: Modern, fast, and automatic documentation generation
- ** Cloud Integration**: Google Cloud Storage for model storage and retrieval
- ** Docker Containerized**: Production-ready container for consistent deployment
- ** Comprehensive Testing**: 77% code coverage with 49+ unit tests
- ** Load Testing**: Locust integration for performance validation
- ** Production Ready**: Health checks, error handling, and monitoring

###  Tech Stack

- **Backend**: FastAPI, Python 3.10, Uvicorn
- **ML Framework**: XGBoost, Scikit-learn, Pandas, NumPy
- **Cloud Platform**: Google Cloud Storage, Cloud Run
- **Containerization**: Docker with multi-layer optimization
- **Testing**: Pytest (unit), Locust (load testing)

## Setup Instructions

### Prerequisites

- Python 3.10+
- Docker Desktop
- Google Cloud SDK (gcloud CLI)
- Git

### Local Development Setup

1. **Clone and Navigate**
```bash
git clone <repository-url>
cd lab3_Rozy_Patel
```

2. **Setup Virtual Environment**
```bash
python -m venv .venv

# Windows
.venv\Scripts\activate

# Linux/Mac
source .venv/bin/activate
```

3. **Install Dependencies**
```bash
pip install -r requirements-minimal.txt
```

4. **Train Model (Optional)**
```bash
python train.py
```

5. **Run API Locally**
```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

6. **Access Endpoints**
- **API Root**: http://localhost:8000
- **Interactive Docs**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health

###  Docker Deployment

```bash
# Build the Docker image
docker build -t penguin-api .

# Run container with GCS integration
docker run -d -p 8080:8080 \
  --name penguin-api-container \
  -v "${PWD}/app/data/penguin-ml-api-*.json:/gcp/sa-key.json:ro" \
  -e GOOGLE_APPLICATION_CREDENTIALS=/gcp/sa-key.json \
  -e GCS_BUCKET_NAME=penguin-models \
  -e GCS_BLOB_NAME=pen_model.json \
  penguin-api
```

###  Cloud Run Deployment

```bash
# Tag for Artifact Registry
docker tag penguin-api us-central1-docker.pkg.dev/penguin-ml-api/penguin-api-repo/penguin-api:latest

# Push to registry
docker push us-central1-docker.pkg.dev/penguin-ml-api/penguin-api-repo/penguin-api:latest

# Deploy to Cloud Run
gcloud run deploy penguin-api \
  --image us-central1-docker.pkg.dev/penguin-ml-api/penguin-api-repo/penguin-api:latest \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --port 8080 \
  --memory 2Gi \
  --cpu 1 \
  --max-instances 100
```

## API Documentation

### Available Endpoints

#### `GET /` - Root Endpoint
Returns API information and status.

**Response:**
```json
{
  "message": "Penguin Species Classification API",
  "version": "1.0.0"
}
```

#### `GET /health` - Health Check
Monitoring endpoint for container orchestration.

**Response:**
```json
{
  "status": "ok"
}
```

#### `POST /predict` - Species Prediction
Main endpoint for penguin species classification.

### ðŸ§ Complete Species Examples

#### Adelie Penguin (Smallest Species)
**Request:**
```json
{
  "bill_length_mm": 39.1,
  "bill_depth_mm": 18.7,
  "flipper_length_mm": 181,
  "body_mass_g": 3750,
  "year": 2007,
  "sex": "male",
  "island": "Torgersen"
}
```

**Response:**
```json
{
  "prediction": "Adelie"
}
```

#### Gentoo Penguin (Largest Species)
**Request:**
```json
{
  "bill_length_mm": 50.0,
  "bill_depth_mm": 15.2,
  "flipper_length_mm": 230,
  "body_mass_g": 6050,
  "year": 2008,
  "sex": "male",
  "island": "Biscoe"
}
```

**Response:**
```json
{
  "prediction": "Gentoo"
}
```

#### Chinstrap Penguin (Medium Species)
**Request:**
```json
{
  "bill_length_mm": 46.5,
  "bill_depth_mm": 17.9,
  "flipper_length_mm": 192,
  "body_mass_g": 3500,
  "year": 2009,
  "sex": "female",
  "island": "Dream"
}
```

**Response:**
```json
{
  "prediction": "Chinstrap"
}
```

### Input Validation Schema

| Field | Type | Constraints | Description |
|-------|------|-------------|-------------|
| `bill_length_mm` | float | 30.0 - 60.0 | Bill length in millimeters |
| `bill_depth_mm` | float | 10.0 - 25.0 | Bill depth in millimeters |
| `flipper_length_mm` | int | 170 - 240 | Flipper length in millimeters |
| `body_mass_g` | int | 2500 - 7000 | Body mass in grams |
| `year` | int | 2007 - 2009 | Year of observation |
| `sex` | enum | "male", "female" | Penguin sex |
| `island` | enum | "Torgersen", "Biscoe", "Dream" | Island location |

---

##  Production Questions & Answers

### 1. What edge cases might break your model in production that aren't in your training data?

**Critical Edge Cases:**

** Data Distribution Shifts:**
- **Climate Change Effects**: Penguins adapting to warming temperatures may have different physical characteristics
- **New Populations**: Penguins from unexplored islands with different genetic traits
- **Seasonal Variations**: Measurements during molting periods vs. breeding seasons
- **Age Demographics**: Only juvenile or only elderly penguins (training data mixed ages)

** Extreme Measurements:**
- **Hybrid Species**: Cross-breeding between species (rare but possible)
- **Injured/Deformed Penguins**: Missing flippers, damaged bills affecting measurements
- **Measurement Errors**: Faulty equipment producing impossible values (negative weights, etc.)
- **Unit Confusion**: Mixing metric/imperial units in input data

** Temporal Shifts:**
- **Evolution**: Long-term genetic changes over decades
- **Population Bottlenecks**: Reduced genetic diversity affecting physical traits
- **Environmental Stressors**: Pollution, food scarcity affecting growth patterns

**Mitigation Strategy:**
```python
def detect_outliers(features: PenguinFeatures) -> bool:
    """Detect potentially problematic inputs"""
    # Statistical outlier detection
    z_scores = calculate_z_scores(features)
    if any(abs(z) > 4 for z in z_scores):
        return True  # Flag for manual review
    
    # Domain knowledge checks
    if features.bill_length_mm < features.bill_depth_mm:
        return True  # Physically unlikely
    
    return False
```

### 2. What happens if your model file becomes corrupted?

**Current Failure Handling:**

** Corruption Detection:**
```python
def validate_model_integrity(model_path: str) -> bool:
    """Validate model file before loading"""
    try:
        # Check file size
        if os.path.getsize(model_path) < 1000:  # Too small
            return False
        
        # Attempt to load model
        model = xgb.XGBClassifier()
        model.load_model(model_path)
        
        # Test prediction on sample data
        test_input = np.array([[39.1, 18.7, 181, 3750, 2007, 1, 0, 1, 0, 0]])
        prediction = model.predict(test_input)
        
        return len(prediction) > 0
    except Exception as e:
        logging.error(f"Model validation failed: {e}")
        return False
```

** Recovery Mechanisms:**

1. **Automatic Fallback**: GCS â†’ Local â†’ Error state
2. **Model Backup Strategy**:
```python
# Multiple model versions stored
MODEL_VERSIONS = [
    "pen_model_v1.json",    # Current
    "pen_model_v0.json",    # Previous stable
    "pen_model_backup.json"  # Emergency fallback
]
```

3. **Health Check Integration**:
```python
@app.get("/health")
async def health_check():
    model_status = validate_model_integrity(MODEL_PATH)
    return {
        "status": "ok" if model_status else "degraded",
        "model_health": model_status,
        "timestamp": datetime.utcnow()
    }
```

4. **Monitoring & Alerts**:
- Model validation runs every 5 minutes
- Automated alerts on corruption detection
- Automatic redeployment from backup

### 3. What's a realistic load for a penguin classification service?

**Load Analysis by Use Case:**

** Research Institution:**
```
Daily Load:
â”œâ”€â”€ Peak Hours (9 AM - 5 PM): 50-200 requests/hour
â”œâ”€â”€ Off Hours: 5-20 requests/hour
â”œâ”€â”€ Daily Total: ~800-1,500 requests
â””â”€â”€ Concurrent Users: 5-15
```

** Educational Platform:**
```
Academic Year Load:
â”œâ”€â”€ Class Sessions: 100-500 requests/hour (burst)
â”œâ”€â”€ Assignment Periods: 1,000-5,000 requests/day
â”œâ”€â”€ Exam Periods: 10,000+ requests/day (spike)
â””â”€â”€ Summer Break: 10-50 requests/day
```

** Public API Service:**
```
Commercial Load:
â”œâ”€â”€ Free Tier: 10,000 requests/month per user
â”œâ”€â”€ Premium Tier: 100,000 requests/month per user
â”œâ”€â”€ Enterprise: 1M+ requests/month
â”œâ”€â”€ Peak RPS: 100-500 during business hours
â””â”€â”€ Geographic Distribution: 60% US, 25% EU, 15% APAC
```

** Realistic Baseline Estimate:**
- **Normal Load**: 10-50 RPS
- **Peak Load**: 100-200 RPS
- **Burst Capacity**: 500+ RPS (auto-scaling)
- **Daily Requests**: 50,000-500,000

### 4. How would you optimize if response times are too slow?

**Performance Optimization Strategy:**

** Level 1: Application Optimization**
```python
# Model caching and connection pooling
class ModelManager:
    def __init__(self):
        self.model = None
        self.gcs_client = None
        self.last_loaded = None
    
    def get_cached_model(self):
        """Keep model in memory"""
        if self.model is None:
            self.model = self.load_model_from_gcs()
        return self.model
    
    def get_pooled_gcs_client(self):
        """Reuse GCS connections"""
        if self.gcs_client is None:
            self.gcs_client = storage.Client()
        return self.gcs_client
```

** Level 2: Infrastructure Optimization**
```bash
# Increase Cloud Run resources
gcloud run deploy penguin-api \
  --cpu 2 \
  --memory 4Gi \
  --concurrency 1000 \
  --min-instances 2  # Reduce cold starts
```

** Level 3: Caching Layer**
```python
# Redis caching for frequent predictions
import redis
import hashlib

cache = redis.Redis(host='redis-url')

def cached_predict(features: PenguinFeatures):
    # Create cache key from input
    cache_key = hashlib.md5(str(features).encode()).hexdigest()
    
    # Check cache first
    cached_result = cache.get(cache_key)
    if cached_result:
        return json.loads(cached_result)
    
    # Compute and cache result
    result = model.predict(features)
    cache.setex(cache_key, 3600, json.dumps(result))  # 1 hour TTL
    return result
```

** Level 4: Advanced Optimizations**
- **Async Processing**: FastAPI native async support
- **Model Quantization**: Reduce XGBoost model size
- **Batch Processing**: Process multiple predictions together
- **CDN Integration**: Static asset caching
- **Load Balancer**: Geographic traffic distribution

### 5. What metrics matter most for ML inference APIs?

** Critical Performance Metrics:**

** Latency Metrics:**
```python
# Response time tracking
@app.middleware("http")
async def track_response_time(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    
    # Log metrics
    logging.info(f"Request: {request.url.path}, Time: {process_time:.3f}s")
    response.headers["X-Process-Time"] = str(process_time)
    return response
```

**Key Latency Metrics:**
- **P50 Response Time**: <100ms (median user experience)
- **P95 Response Time**: <500ms (95% of users)
- **P99 Response Time**: <1000ms (handle edge cases)
- **Cold Start Time**: <3s (Cloud Run warmup)

** Accuracy Metrics:**
```python
# Model performance monitoring
class ModelMetrics:
    def track_prediction_confidence(self, prediction_proba):
        confidence = max(prediction_proba)
        if confidence < 0.7:  # Low confidence threshold
            self.alert_low_confidence()
    
    def track_prediction_distribution(self, predictions):
        # Monitor for data drift
        species_distribution = Counter(predictions)
        expected_distribution = {"Adelie": 0.44, "Gentoo": 0.36, "Chinstrap": 0.20}
        
        if self.distribution_drift(species_distribution, expected_distribution):
            self.alert_data_drift()
```

** Business Metrics:**
- **Throughput**: Requests per second (RPS)
- **Availability**: 99.9% uptime SLA
- **Error Rate**: <1% for valid requests
- **Cost per Prediction**: Cloud Run pricing optimization

** Operational Metrics:**
- **Memory Usage**: Container resource utilization
- **CPU Usage**: Compute efficiency
- **Instance Count**: Auto-scaling behavior
- **Cache Hit Rate**: Performance optimization effectiveness

### 6. Why is Docker layer caching important for build speed? (Did you leverage it?)

** Docker Layer Caching Benefits:**

** Build Speed Optimization:**
My Dockerfile leverages layer caching effectively:

```dockerfile
# GOOD: Dependencies cached separately from code
FROM python:3.10-slim

# Layer 1: System dependencies (rarely change)
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential gcc curl \
    && rm -rf /var/lib/apt/lists/*

# Layer 2: Python dependencies (change occasionally)
COPY requirements-minimal.txt .
RUN pip install --upgrade pip && \
    pip install -r requirements-minimal.txt

# Layer 3: Application code (changes frequently)
COPY . .
```

** Caching Strategy Analysis:**

** Leveraged Optimizations:**
- **Base Image Caching**: `python:3.10-slim` pulled once
- **System Dependencies**: `apt-get` layer cached until Dockerfile changes
- **Python Dependencies**: Requirements cached until `requirements-minimal.txt` changes
- **Multi-stage Potential**: Could separate build/runtime environments

** Missed Opportunities:**
```dockerfile
# BETTER: Separate requirements copy
COPY requirements-minimal.txt /app/
WORKDIR /app
RUN pip install -r requirements-minimal.txt

# THEN copy application code
COPY app/ /app/app/
COPY train.py upload_model_to_gcs.py ./
```

** Build Time Comparison:**
- **First Build**: ~3-5 minutes (download base image + dependencies)
- **Code Changes**: ~30 seconds (only rebuild app layer)
- **Dependency Changes**: ~2-3 minutes (rebuild from pip install)
- **Infrastructure Changes**: ~3-5 minutes (full rebuild)

** Advanced Caching Strategies:**
```dockerfile
# Multi-stage build for production
FROM python:3.10-slim as builder
RUN pip install --user xgboost fastapi uvicorn

FROM python:3.10-slim as runtime
COPY --from=builder /root/.local /root/.local
COPY app/ /app/
```

### 7. What security risks exist with running containers as root?

** Root Container Security Risks:**

** Privilege Escalation:**
- **Container Breakout**: Root inside = root on host (if kernel exploit)
- **File System Access**: Can modify host files if volumes mounted
- **Process Control**: Can kill host processes through shared PID namespace
- **Network Access**: Can bind to privileged ports, sniff network traffic

** My Current Risk Assessment:**

** Current Dockerfile runs as root:**
```dockerfile
# This runs as root user (UID 0)
FROM python:3.10-slim
# No USER directive = root user
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8080"]
```

** Improved Security Implementation:**
```dockerfile
# Create non-root user
FROM python:3.10-slim

# Create app user and group
RUN groupadd -r appgroup && useradd -r -g appgroup appuser

# Install dependencies as root
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential gcc curl

# Copy and install Python packages
COPY requirements-minimal.txt .
RUN pip install -r requirements-minimal.txt

# Copy application code
COPY --chown=appuser:appgroup . /app
WORKDIR /app

# Switch to non-root user
USER appuser

# Run as non-root
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8080"]
```

** Additional Security Measures:**
```dockerfile
# Read-only root filesystem
FROM python:3.10-slim
RUN adduser --disabled-password --gecos '' appuser
USER appuser
# Make filesystem read-only
VOLUME ["/tmp"]  # Only /tmp writable
```

** Cloud Run Security:**
Cloud Run provides additional isolation:
- **gVisor Sandboxing**: Extra layer between container and kernel
- **Network Isolation**: Containers can't reach other services by default
- **IAM Integration**: Fine-grained access control
- **Automatic Security Updates**: Google manages base image patches

### 8. How does cloud auto-scaling affect your load test results?

** Auto-Scaling Impact on Load Testing:**

** Cloud Run Auto-Scaling Behavior:**

**Cold Start Effects:**
```
Load Test Scenario: 1 â†’ 50 users over 2 minutes

Timeline:
0-30s:  Cold start (2-3s latency) â†’ 1 instance
30-60s: Warm instances (100ms latency) â†’ 5 instances
60-90s: Scale up (200ms latency) â†’ 15 instances
90-120s: Steady state (100ms latency) â†’ 20 instances
```

** Observed Load Test Patterns:**

** Baseline Test (1 user, 60s):**
- **Result**: Single warm instance, consistent 100ms response
- **Auto-scaling**: No scaling triggered
- **Realistic**: Represents single user experience

** Normal Load (10 users, 5m):**
- **Result**: 2-3 instances, avg 150ms response
- **Auto-scaling**: Minimal scaling, mostly warm instances
- **Realistic**: Represents normal business load

** Stress Test (50 users, 2m):**
- **Result**: 10-15 instances, avg 300ms response (including scale-up)
- **Auto-scaling**: Active scaling throughout test
- **Realistic**: Results skewed by scaling overhead

** Spike Test (1â†’100 users, 1m):**
- **Result**: High latency (1-5s) during ramp-up, then stabilizes
- **Auto-scaling**: Aggressive scaling, many cold starts
- **Realistic**: Not representative of steady-state performance

** Load Test Result Interpretation:**

**Misleading Metrics:**
- **Average Response Time**: Skewed by cold starts
- **Peak RPS**: Limited by scaling speed, not capacity
- **Error Rates**: Timeouts during rapid scaling

**Accurate Metrics:**
- **Steady-State Performance**: After 2-3 minutes of constant load
- **Per-Instance Throughput**: Individual container capacity
- **Scaling Speed**: Time to reach target capacity

** Improved Load Testing Strategy:**
```python
# Pre-warm instances before testing
def pre_warm_instances(target_rps: int, duration: int):
    """Send traffic to warm up instances before main test"""
    warm_up_period = 300  # 5 minutes
    warm_up_rps = target_rps * 0.5
    
    # Gradual ramp to warm instances
    for i in range(warm_up_period):
        current_rps = (warm_up_rps * i) / warm_up_period
        send_requests(current_rps)
        time.sleep(1)
```

### 9. What would happen with 10x more traffic?

** 10x Traffic Impact Analysis:**

**Current Capacity:**
- **Normal Load**: 50 RPS, 10 concurrent users
- **10x Load**: 500 RPS, 100 concurrent users

** Cloud Run Auto-Scaling Response:**

**Resource Requirements:**
```
Current: 50 RPS
â”œâ”€â”€ Instances: 2-3
â”œâ”€â”€ Memory: 2Gi per instance
â”œâ”€â”€ CPU: 1 vCPU per instance
â””â”€â”€ Response Time: 100-200ms

10x Load: 500 RPS
â”œâ”€â”€ Instances: 20-30 (estimated)
â”œâ”€â”€ Memory: 40-60Gi total
â”œâ”€â”€ CPU: 20-30 vCPU total
â””â”€â”€ Response Time: 200-500ms (with scaling overhead)
```

**âš  Potential Bottlenecks:**

**1. GCS API Limits:**
```python
# Current: ~50 model downloads/minute from GCS
# 10x Load: ~500 model downloads/minute
# GCS Limit: 5,000 requests/minute (safe)
# Solution: Model caching eliminates this bottleneck
```

**2. Memory Constraints:**
```
Per-Instance Memory Usage:
â”œâ”€â”€ Base Python: 50MB
â”œâ”€â”€ ML Libraries: 200MB
â”œâ”€â”€ Model in Memory: 50MB
â”œâ”€â”€ Request Processing: 100MB
â””â”€â”€ Total per Instance: ~400MB

At 30 instances: 12GB total memory usage
Cloud Run Limit: Virtually unlimited (pay-per-use)
```

**3. Cold Start Cascades:**
```
Problem: Rapid scaling triggers many cold starts
Impact: 2-3 second delays for 30-50% of requests
Solution: Minimum instance configuration
```

**ðŸ› ï¸ Mitigation Strategies:**

**Immediate (No Code Changes):**
```bash
# Increase instance limits and pre-warm
gcloud run deploy penguin-api \
  --min-instances 10 \      # Pre-warmed instances
  --max-instances 200 \     # Higher ceiling
  --concurrency 100 \       # More requests per instance
  --cpu 2 \                 # More CPU per instance
  --memory 4Gi              # More memory per instance
```

**Short-term (Application Changes):**
```python
# Implement request batching
@app.post("/predict_batch")
async def predict_batch(features_list: List[PenguinFeatures]):
    """Process multiple predictions in one request"""
    predictions = []
    for features in features_list:
        prediction = model.predict(features)
        predictions.append(prediction)
    return {"predictions": predictions}
```

**Long-term (Architecture Changes):**
- **Load Balancer**: Multiple regional deployments
- **Redis Caching**: Cache frequent predictions
- **Model Serving**: Dedicated ML serving infrastructure (Vertex AI)
- **CDN**: Geographic content distribution

### 10. How would you monitor performance in production?

** Comprehensive Production Monitoring:**

** Application-Level Monitoring:**

**Custom Metrics Collection:**
```python
import time
from prometheus_client import Counter, Histogram, generate_latest

# Metrics definitions
REQUEST_COUNT = Counter('api_requests_total', 'Total API requests', ['method', 'endpoint', 'status'])
REQUEST_DURATION = Histogram('api_request_duration_seconds', 'Request duration')
PREDICTION_CONFIDENCE = Histogram('model_prediction_confidence', 'Model confidence scores')
MODEL_ERRORS = Counter('model_errors_total', 'Model prediction errors')

@app.middleware("http")
async def monitor_requests(request: Request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    # Record metrics
    duration = time.time() - start_time
    REQUEST_COUNT.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).inc()
    REQUEST_DURATION.observe(duration)
    
    return response

@app.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint"""
    return Response(generate_latest(), media_type="text/plain")
```

** Business Logic Monitoring:**
```python
class PredictionMonitor:
    def __init__(self):
        self.predictions_buffer = []
        self.last_drift_check = time.time()
    
    def track_prediction(self, features: PenguinFeatures, prediction: str, confidence: float):
        """Track individual predictions"""
        self.predictions_buffer.append({
            'timestamp': time.time(),
            'prediction': prediction,
            'confidence': confidence,
            'features': features.dict()
        })
        
        # Alert on low confidence
        if confidence < 0.7:
            self.alert_low_confidence(features, prediction, confidence)
    
    def check_data_drift(self):
        """Monitor for distribution changes"""
        if time.time() - self.last_drift_check > 3600:  # Hourly
            recent_predictions = self.get_recent_predictions(hours=24)
            baseline_distribution = {"Adelie": 0.44, "Gentoo": 0.36, "Chinstrap": 0.20}
            
            current_distribution = self.calculate_distribution(recent_predictions)
            
            if self.detect_significant_drift(baseline_distribution, current_distribution):
                self.alert_data_drift(current_distribution)
            
            self.last_drift_check = time.time()
```

** Infrastructure Monitoring:**

**Google Cloud Monitoring Integration:**
```yaml
# monitoring.yaml
resources:
  - name: penguin-api-alerts
    type: monitoring.v1.alertPolicy
    properties:
      displayName: "Penguin API Performance Alerts"
      conditions:
        - displayName: "High Response Latency"
          conditionThreshold:
            filter: 'resource.type="cloud_run_revision"'
            comparison: COMPARISON_GREATER_THAN
            thresholdValue: 1000  # 1 second
        - displayName: "High Error Rate"
          conditionThreshold:
            filter: 'resource.type="cloud_run_revision"'
            comparison: COMPARISON_GREATER_THAN
            thresholdValue: 0.05  # 5%
        - displayName: "Memory Usage High"
          conditionThreshold:
            filter: 'resource.type="cloud_run_revision"'
            comparison: COMPARISON_GREATER_THAN
            thresholdValue: 1.8  # 1.8GB of 2GB limit
```

** Monitoring Dashboard Stack:**

**Level 1: Real-time Alerting (Immediate Response)**
```python
# Slack/PagerDuty integration
class AlertManager:
    def send_critical_alert(self, message: str):
        """Send immediate alerts for critical issues"""
        webhook_url = os.getenv("SLACK_WEBHOOK_URL")
        payload = {
            "text": f"CRITICAL: Penguin API - {message}",
            "channel": "#production-alerts"
        }
        requests.post(webhook_url, json=payload)
    
    def send_warning_alert(self, message: str):
        """Send warnings for degraded performance"""
        # Log to monitoring system
        logging.warning(f"PERFORMANCE WARNING: {message}")
```

**Level 2: Operational Dashboard (5-minute intervals)**
- **Response Times**: P50, P95, P99 latencies
- **Throughput**: Requests per second, concurrent users
- **Error Rates**: 4xx/5xx breakdown by endpoint
- **Resource Usage**: CPU, memory, instance count
- **Model Performance**: Prediction confidence distribution

**Level 3: Business Intelligence (Daily/Weekly)**
- **Usage Patterns**: Peak hours, geographic distribution
- **Model Accuracy**: Drift detection, confidence trends
- **Cost Analysis**: Per-prediction costs, resource optimization
- **Capacity Planning**: Growth trends, scaling requirements

### 11. How would you implement blue-green deployment?

** Blue-Green Deployment Strategy:**

** Cloud Run Blue-Green Implementation:**

**Phase 1: Parallel Environment Setup**
```bash
# Deploy green version alongside blue
gcloud run deploy penguin-api-green \
  --image us-central1-docker.pkg.dev/penguin-ml-api/penguin-api-repo/penguin-api:v2.0 \
  --platform managed \
  --region us-central1 \
  --no-traffic \  # No traffic initially
  --tag green

# Current blue version continues serving 100% traffic
gcloud run services update-traffic penguin-api \
  --to-revisions blue=100
```

**Phase 2: Green Environment Validation**
```python
# Automated deployment validation script
class DeploymentValidator:
    def __init__(self, green_url: str):
        self.green_url = green_url
        self.test_cases = self.load_test_cases()
    
    def validate_green_deployment(self) -> bool:
        """Comprehensive green environment testing"""
        
        # Health check
        if not self.health_check():
            return False
        
        # Functional testing
        if not self.run_api_tests():
            return False
        
        # Performance testing
        if not self.performance_baseline_test():
            return False
        
        # Model accuracy testing
        if not self.model_accuracy_test():
            return False
        
        return True
    
    def health_check(self) -> bool:
        """Basic health verification"""
        try:
            response = requests.get(f"{self.green_url}/health", timeout=10)
            return response.status_code == 200
        except Exception:
            return False
    
    def run_api_tests(self) -> bool:
        """Run all three species prediction tests"""
        test_cases = [
            {"features": self.adelie_features, "expected": "Adelie"},
            {"features": self.gentoo_features, "expected": "Gentoo"},
            {"features": self.chinstrap_features, "expected": "Chinstrap"}
        ]
        
        for case in test_cases:
            response = requests.post(f"{self.green_url}/predict", json=case["features"])
            if response.json().get("prediction") != case["expected"]:
                return False
        
        return True
    
    def performance_baseline_test(self) -> bool:
        """Ensure green performs as well as blue"""
        # Run 100 requests and measure response time
        times = []
        for _ in range(100):
            start = time.time()
            requests.post(f"{self.green_url}/predict", json=self.sample_features)
            times.append(time.time() - start)
        
        avg_response_time = sum(times) / len(times)
        return avg_response_time < 0.5  # 500ms threshold
```

**Phase 3: Gradual Traffic Migration**
```bash
# Canary deployment: 10% traffic to green
gcloud run services update-traffic penguin-api \
  --to-revisions blue=90,green=10

# Monitor for 15 minutes, then increase if stable
gcloud run services update-traffic penguin-api \
  --to-revisions blue=50,green=50

# Final cutover if all metrics are healthy
gcloud run services update-traffic penguin-api \
  --to-revisions green=100

# Remove blue version after success confirmation
gcloud run revisions delete blue-revision-id
```

**ðŸ› ï¸ Automated Blue-Green Pipeline:**
```yaml
# .github/workflows/blue-green-deploy.yml
name: Blue-Green Deployment

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Deploy Green Version
        run: |
          gcloud run deploy penguin-api \
            --image ${{ env.IMAGE_URL }}:${{ github.sha }} \
            --tag green \
            --no-traffic
      
      - name: Validate Green Environment
        run: |
          python scripts/validate_deployment.py \
            --url ${{ env.GREEN_URL }} \
            --timeout 300
      
      - name: Gradual Traffic Migration
        run: |
          # 10% canary
          gcloud run services update-traffic penguin-api --to-revisions blue=90,green=10
          sleep 300  # 5 minute observation
          
          # 50% split
          gcloud run services update-traffic penguin-api --to-revisions blue=50,green=50
          sleep 600  # 10 minute observation
          
          # Full cutover
          gcloud run services update-traffic penguin-api --to-revisions green=100
      
      - name: Cleanup Old Version
        if: success()
        run: |
          gcloud run revisions delete $(gcloud run revisions list --filter="blue" --format="value(name)")
```

** Deployment Monitoring:**
```python
class BlueGreenMonitor:
    def monitor_deployment(self, blue_url: str, green_url: str, traffic_split: dict):
        """Monitor both versions during deployment"""
        
        metrics = {
            'blue': self.collect_metrics(blue_url),
            'green': self.collect_metrics(green_url)
        }
        
        # Compare error rates
        if metrics['green']['error_rate'] > metrics['blue']['error_rate'] * 1.5:
            self.rollback_deployment()
        
        # Compare response times
        if metrics['green']['avg_response_time'] > metrics['blue']['avg_response_time'] * 1.2:
            self.rollback_deployment()
        
        # Compare prediction accuracy
        if metrics['green']['prediction_confidence'] < metrics['blue']['prediction_confidence'] * 0.95:
            self.rollback_deployment()
```

### 12. What would you do if deployment fails in production?

** Production Deployment Failure Response:**

** Immediate Response (0-5 minutes):**

**Automated Rollback Triggers:**
```python
class DeploymentFailureHandler:
    def __init__(self):
        self.rollback_triggers = {
            'error_rate_threshold': 0.05,  # 5% error rate
            'response_time_threshold': 2.0,  # 2 second response time
            'health_check_failures': 3,  # 3 consecutive failures
            'availability_threshold': 0.99  # 99% availability
        }
    
    def monitor_deployment(self, deployment_id: str):
        """Real-time deployment monitoring"""
        start_time = time.time()
        
        while time.time() - start_time < 600:  # 10 minute window
            metrics = self.collect_current_metrics()
            
            if self.should_rollback(metrics):
                self.execute_emergency_rollback(deployment_id)
                return False
            
            time.sleep(30)  # Check every 30 seconds
        
        return True  # Deployment successful
    
    def should_rollback(self, metrics: dict) -> bool:
        """Determine if immediate rollback is needed"""
        return (
            metrics['error_rate'] > self.rollback_triggers['error_rate_threshold'] or
            metrics['avg_response_time'] > self.rollback_triggers['response_time_threshold'] or
            metrics['health_check_failures'] >= self.rollback_triggers['health_check_failures']
        )
    
    def execute_emergency_rollback(self, deployment_id: str):
        """Immediate rollback to previous stable version"""
        logging.critical(f"EMERGENCY ROLLBACK: Deployment {deployment_id} failed")
        
        # Instant traffic cutover to blue (previous stable)
        subprocess.run([
            "gcloud", "run", "services", "update-traffic", "penguin-api",
            "--to-revisions", "blue=100"
        ])
        
        # Alert all stakeholders
        self.send_critical_alert(f"Production rollback executed for deployment {deployment_id}")
```

** Manual Rollback Process:**
```bash
# Emergency rollback commands (keep handy)

# 1. Immediate traffic cutover to stable version
gcloud run services update-traffic penguin-api --to-revisions blue=100

# 2. Check service status
gcloud run services describe penguin-api --region us-central1

# 3. Verify rollback success
curl https://penguin-api-url/health

# 4. Delete failed deployment
gcloud run revisions delete failed-revision-id
```

** Failure Investigation (5-30 minutes):**

**Automated Diagnostics:**
```python
class FailureInvestigator:
    def diagnose_deployment_failure(self, failed_deployment_id: str):
        """Comprehensive failure analysis"""
        
        # Collect logs from failed deployment
        logs = self.get_deployment_logs(failed_deployment_id)
        
        # Analyze common failure patterns
        failure_analysis = {
            'container_startup_failure': self.check_startup_errors(logs),
            'dependency_failure': self.check_dependency_errors(logs),
            'model_loading_failure': self.check_model_errors(logs),
            'resource_constraints': self.check_resource_errors(logs),
            'configuration_issues': self.check_config_errors(logs)
        }
        
        # Generate failure report
        self.generate_failure_report(failure_analysis)
        
        return failure_analysis
    
    def check_startup_errors(self, logs: list) -> dict:
        """Check for container startup issues"""
        startup_errors = []
        
        for log in logs:
            if 'failed to start' in log.lower():
                startup_errors.append(log)
            if 'port already in use' in log.lower():
                startup_errors.append(log)
            if 'permission denied' in log.lower():
                startup_errors.append(log)
        
        return {
            'found_errors': len(startup_errors) > 0,
            'error_count': len(startup_errors),
            'sample_errors': startup_errors[:5]
        }
```

** Common Failure Scenarios & Solutions:**

**1. Container Build Failure:**
```bash
# Problem: Docker build fails
# Quick Fix: Use previous working image
docker tag penguin-api:previous-stable penguin-api:latest
docker push us-central1-docker.pkg.dev/penguin-ml-api/penguin-api-repo/penguin-api:latest

# Root Cause Analysis:
- Check Dockerfile syntax
- Verify base image availability
- Check dependency versions in requirements.txt
```

**2. Model Loading Failure:**
```python
# Problem: Model file corrupted or inaccessible
# Quick Fix: Force local model fallback
os.environ["GCS_BUCKET_NAME"] = ""  # Disable GCS, use local model

# Root Cause Analysis:
def diagnose_model_failure():
    # Check GCS bucket access
    try:
        client = storage.Client()
        bucket = client.bucket("penguin-models")
        blob = bucket.blob("pen_model.json")
        if not blob.exists():
            return "Model file missing from GCS"
    except Exception as e:
        return f"GCS access failed: {e}"
    
    # Check model file integrity
    try:
        with tempfile.NamedTemporaryFile() as temp_file:
            blob.download_to_filename(temp_file.name)
            model = xgb.XGBClassifier()
            model.load_model(temp_file.name)
            return "Model file is valid"
    except Exception as e:
        return f"Model file corrupted: {e}"
```

**3. Resource Exhaustion:**
```bash
# Problem: Container runs out of memory/CPU
# Quick Fix: Increase resources temporarily
gcloud run deploy penguin-api \
  --cpu 4 \
  --memory 8Gi \
  --max-instances 200

# Long-term Fix: Optimize application memory usage
```

** Recovery Verification:**
```python
def verify_recovery(service_url: str) -> bool:
    """Verify service is fully recovered"""
    
    # Health check
    health_response = requests.get(f"{service_url}/health")
    if health_response.status_code != 200:
        return False
    
    # Functional test
    test_prediction = {
        "bill_length_mm": 39.1,
        "bill_depth_mm": 18.7,
        "flipper_length_mm": 181,
        "body_mass_g": 3750,
        "year": 2007,
        "sex": "male",
        "island": "Torgersen"
    }
    
    prediction_response = requests.post(f"{service_url}/predict", json=test_prediction)
    if prediction_response.status_code != 200:
        return False
    
    result = prediction_response.json()
    if "prediction" not in result:
        return False
    
    # Performance check
    start_time = time.time()
    requests.post(f"{service_url}/predict", json=test_prediction)
    response_time = time.time() - start_time
    
    if response_time > 1.0:  # Response time should be under 1 second
        return False
    
    return True
```

### 13. What happens if your container uses too much memory?

** Memory Management in Production:**

** Memory Exhaustion Scenarios:**

**Container Memory Limits:**
```yaml
# Cloud Run Memory Configuration
resources:
  limits:
    memory: 2Gi    # Hard limit per instance
    cpu: 1         # 1 vCPU per instance

# What happens when limit is exceeded:
# 1. Container gets OOMKilled (Out of Memory)
# 2. Cloud Run restarts the container
# 3. Requests fail with 503 Service Unavailable
# 4. Auto-scaler may spin up new instances
```

** Memory Usage Analysis:**

**Current Memory Footprint:**
```python
import psutil
import sys

def analyze_memory_usage():
    """Track application memory consumption"""
    
    process = psutil.Process()
    memory_info = process.memory_info()
    
    return {
        'rss_mb': memory_info.rss / 1024 / 1024,  # Resident memory
        'vms_mb': memory_info.vms / 1024 / 1024,  # Virtual memory
        'percent': process.memory_percent(),
        'available_mb': psutil.virtual_memory().available / 1024 / 1024
    }

# Typical memory usage in production:
BASE_PYTHON = 50MB       # Python interpreter
ML_LIBRARIES = 200MB     # XGBoost, pandas, numpy
MODEL_IN_MEMORY = 50MB   # Loaded XGBoost model
REQUEST_PROCESSING = 10-50MB  # Per concurrent request
TOTAL_BASELINE = ~310MB  # Minimum memory usage
```

** Memory Leak Detection:**
```python
class MemoryMonitor:
    def __init__(self):
        self.memory_samples = []
        self.baseline_memory = None
        
    def track_memory_usage(self):
        """Track memory usage over time"""
        current_memory = psutil.Process().memory_info().rss
        self.memory_samples.append({
            'timestamp': time.time(),
            'memory_mb': current_memory / 1024 / 1024
        })
        
        # Keep only last 100 samples
        if len(self.memory_samples) > 100:
            self.memory_samples.pop(0)
        
        # Set baseline on first measurement
        if self.baseline_memory is None:
            self.baseline_memory = current_memory
        
        # Check for memory leak (>50% increase from baseline)
        if current_memory > self.baseline_memory * 1.5:
            self.alert_memory_leak(current_memory)
    
    def alert_memory_leak(self, current_memory: int):
        """Alert on potential memory leak"""
        logging.warning(f"MEMORY LEAK DETECTED: {current_memory / 1024 / 1024:.1f}MB")
        
        # Force garbage collection
        import gc
        gc.collect()
        
        # Check if memory drops after GC
        post_gc_memory = psutil.Process().memory_info().rss
        if post_gc_memory < current_memory * 0.9:
            logging.info("Memory cleaned up successfully")
        else:
            logging.error("Memory leak persists after garbage collection")

# Integrate with request middleware
@app.middleware("http")
async def memory_monitoring_middleware(request: Request, call_next):
    memory_monitor.track_memory_usage()
    response = await call_next(request)
    return response
```

** Memory Optimization Strategies:**

**Level 1: Application Optimization**
```python
# Model caching optimization
class OptimizedModelManager:
    def __init__(self):
        self._model = None
        self._model_lock = asyncio.Lock()
    
    async def get_model(self):
        """Thread-safe model loading with memory optimization"""
        if self._model is None:
            async with self._model_lock:
                if self._model is None:  # Double-check locking
                    self._model = self.load_optimized_model()
        return self._model
    
    def load_optimized_model(self):
        """Load model with memory optimizations"""
        import gc
        
        # Force cleanup before loading
        gc.collect()
        
        # Load model
        model = xgb.XGBClassifier()
        model.load_model(self.model_path)
        
        # Optimize model for inference
        # (XGBoost models are already optimized, but other models might benefit)
        
        return model

# Request-level memory management
@app.post("/predict")
async def predict(features: PenguinFeatures):
    try:
        # Process prediction
        result = await process_prediction(features)
        return result
    finally:
        # Explicit cleanup for large requests
        import gc
        gc.collect()
```

**Level 2: Infrastructure Scaling**
```bash
# Increase memory limits if application legitimately needs more
gcloud run deploy penguin-api \
  --memory 4Gi \           # Double the memory
  --concurrency 50 \       # Reduce concurrent requests per instance
  --max-instances 200      # Allow more instances to handle load
```

**Level 3: Advanced Memory Management**
```python
# Memory-mapped model loading for large models
import mmap

class MemoryMappedModel:
    def __init__(self, model_path: str):
        self.model_path = model_path
        self._memory_map = None
        self._model = None
    
    def load_model_memory_mapped(self):
        """Load model using memory mapping to reduce RAM usage"""
        with open(self.model_path, 'rb') as f:
            self._memory_map = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)
            # Note: XGBoost doesn't directly support memory mapping
            # This is more applicable to other model formats
```

** Container Recovery Strategies:**

**Automatic Recovery:**
```yaml
# Cloud Run automatic restart behavior
spec:
  template:
    spec:
      containers:
      - image: penguin-api:latest
        resources:
          limits:
            memory: 2Gi
        # Cloud Run automatically:
        # 1. Detects OOM kill
        # 2. Restarts container
        # 3. Routes traffic to healthy instances
        # 4. Logs incident for analysis
```

**Graceful Degradation:**
```python
class MemoryPressureHandler:
    def __init__(self, memory_threshold_mb: int = 1800):  # 1.8GB of 2GB limit
        self.memory_threshold = memory_threshold_mb * 1024 * 1024
        
    def check_memory_pressure(self) -> bool:
        """Check if memory usage is approaching limits"""
        current_memory = psutil.Process().memory_info().rss
        return current_memory > self.memory_threshold
    
    async def handle_memory_pressure(self):
        """Graceful degradation under memory pressure"""
        if self.check_memory_pressure():
            # Clear caches
            self.clear_prediction_cache()
            
            # Force garbage collection
            import gc
            gc.collect()
            
            # Reject new requests temporarily
            raise HTTPException(
                status_code=503,
                detail="Service temporarily unavailable due to memory pressure"
            )

@app.middleware("http")
async def memory_pressure_middleware(request: Request, call_next):
    memory_handler = MemoryPressureHandler()
    
    if memory_handler.check_memory_pressure():
        await memory_handler.handle_memory_pressure()
    
    return await call_next(request)
```

** Memory Monitoring Dashboard:**
```python
# Memory metrics for monitoring
@app.get("/metrics/memory")
async def memory_metrics():
    """Expose memory metrics for monitoring"""
    process = psutil.Process()
    memory_info = process.memory_info()
    
    return {
        "memory_usage_mb": memory_info.rss / 1024 / 1024,
        "memory_percent": process.memory_percent(),
        "available_memory_mb": psutil.virtual_memory().available / 1024 / 1024,
        "memory_limit_mb": 2048,  # Cloud Run limit
        "memory_pressure": memory_info.rss > (1800 * 1024 * 1024)  # 90% of limit
    }
```

This comprehensive approach ensures that memory issues are detected early, handled gracefully, and resolved quickly to maintain service availability.

---

## Project Structure

```
lab3_Rozy_Patel/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ pen_model.json              # Trained XGBoost model
â”‚   â”‚   â”œâ”€â”€ columns.json                # Feature columns
â”‚   â”‚   â”œâ”€â”€ label_classes.json          # Species labels
â”‚   â”‚   â””â”€â”€ penguin-ml-api-*.json       # GCS service account
â”‚   â””â”€â”€ main.py                         # FastAPI application
â”œâ”€â”€ tests/                              # Test suite (77% coverage)
â”œâ”€â”€ train.py                            # Model training script
â”œâ”€â”€ upload_model_to_gcs.py              # GCS upload utility
â”œâ”€â”€ locustfile.py                       # Load testing scenarios
â”œâ”€â”€ Dockerfile                          # Container configuration
â”œâ”€â”€ requirements-minimal.txt            # Python dependencies
â”œâ”€â”€ DEPLOYMENT.md                       # Deployment documentation
â””â”€â”€ README.md                           # This comprehensive guide
```

## Quick Testing Commands

```bash
# Complete test suite
pytest tests/ --cov=app --cov-report=html -v

# Test all species examples
python test_species_examples.py

# Load testing
locust --web-port 8090

# Docker container test
docker run -d -p 8080:8080 --name test-container penguin-api
curl http://localhost:8080/health
```

---
